#!/usr/bin/env python
# coding: utf-8

# 

# 

# 

# In[1]:


import paddle.fluid as fluid
import paddle
import numpy as np
import os
import json
import sys
import matplotlib.pyplot as plt
import pandas as pd
import os


# import pandas as pd
# 
# df = pd.read_excel('file1.xlsx', sheetname='Sheet1', header=None)		# 使用pandas模块读取数据
# print('开始写入txt文件...')
# df.to_csv('file2.txt', header=None, sep=',', index=False)		# 写入，逗号分隔
# print('文件写入成功!')
# 

# <h2><font face="SimSun" color="black">1.2 飞桨基础接口</font></h2>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;关于使用飞桨基础接口定义数据读取器（API文档底部有相关的调用样例，可供大家递归参考）。</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（1）[数据集定义与加载](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/02_paddle2.0_develop/02_data_load_cn.html)；</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（2）[开发流程 - 数据处理](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/01_paddle2.0_introduction/update_cn.html#shujuchuli)；</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（3）[paddle.io.Dataset](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html)；</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（4）[paddle.io.DataLoader](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html)；</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（5）[飞桨文档搜索*](https://www.paddlepaddle.org.cn/searchall?q=&language=zh&version=2.2)；</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（6）[开发者案例 - Paddle2.0 数据加载与处理*](https://aistudio.baidu.com/aistudio/projectdetail/1349615)；</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（7）[开发者案例 - 反欺诈竞赛数据处理（表格数据）*](https://aistudio.baidu.com/aistudio/projectdetail/1191896)；</font>

# <h1><font face="SimSun" color="black">2 作业书写区</font></h1>
# 
# <font face="SimSun" color="black" size="4">&emsp;&emsp;**评分标准：**</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（1）数据集清洗与介绍（解压数据集、tree命令查看目录结构、样本的可视化展示等），30分。</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（2）**图像数据集**：计算该数据集的均值和方差；**文本数据集**：使用jieba分词并统计词频，30分。</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（3）数据集类的定义（继承paddle.io.Dataset的类），30分。</font>
# 
# <font face="SimSun" color="black" size="3">&emsp;&emsp;（4）数据集类的测试（调用定义好的数据集类，参考章节[1.2]的文档示例），10分。</font>
# 
# ---

# In[2]:


# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
get_ipython().system('ls /home/aistudio/data')


# In[3]:


get_ipython().system('python3 -c "import paddle; print(paddle.__version__)"')


# <h2><font face="SimSun" color="black">2.1 数据加载（30分）</font></h2>

# #数据集在另一个项目，我这里直接将csv上传到price中

# In[4]:


#注意加点，就是当前目录

# 解压数据集
# !unzip -oq ./data/data108061/price.zip


# In[5]:


import os
#我的脚本在/home/zzq下面 先切换过去
os.chdir("./price");


# In[6]:


print (os.path.abspath('.'))#获得当前工作目录,切换到了price文件夹下


# ['身份'] + ['在建筑中'] + ['是否可分期'] + ['卧室数量'] + ['含卧室或大厅'] + ['平米数'] + ['是否拎包入住'] + ['是否零售'] + ['经度'] + ['纬度'] + ['房价'] 一共十个特征，一个标签

# <h2><font face="SimSun" color="black">2.2 图像/文本数据的统计分析（30分）</font></h2>

# In[8]:


import sys

path = "train.txt"  # 数据来源
f = open(path , encoding='utf-8')
line = f.readline()
list = []
while line:
    a = line.split(",")
    b = a[:8] + a[-3:]
    list.append(b)
    line = f.readline()
f.close()

print(list)

with open('train6.txt', 'a') as month_file:  # 提取后的数据文件
    for line in list:
        s = ','.join(line)
        month_file.write(s)


# In[9]:


# ['身份'] + ['在建筑中'] +['身份'] +['平米数'] + ['竣工日期'] +['是否零售'] + ['位置'] + ['经度'] +
colnames = ['身份'] + ['在建筑中'] + ['是否可分期'] + ['卧室数量'] + ['含卧室或大厅'] + ['平米数'] + ['是否拎包入住'] + ['是否零售'] +  ['经度'] + ['纬度'] + ['房价']
print_data = pd.read_csv('train6.txt',names = colnames)
print_data.head()


# <h2><font face="SimSun" color="black">2.3 数据集类的定义（30分）</font></h2>

# In[11]:


# sys.path.append("/home/aistudio/price/")

# 读入训练数据
# datafile = 'train.txt'
# data = np.fromfile(datafile, dtype=np.float32)
def iris_type(s):
    it = {b'Owner':0, b'Dealer':1, b'Builder':2}
    return it[s]
def iris_type2(s):
    it = {b'BHK':0, b'RK':1}
    return it[s]

data_train = np.loadtxt('train7.txt',delimiter=',', converters = {0:iris_type,4:iris_type2}) 
# data_test =  np.loadtxt('test2.txt',delimiter=',', converters = {0:iris_type,4:iris_type2}) 
print(data_train)
print(type(data_train))
print(np.shape(data_train))
#此时的数据还未归一化
# 打印数据
# with open('train.txt') as file_obj:
#     content = file_obj.read()
#     print(content)


# 打印出的结果
# [[ 0.        0.        0.       ... 12.96991  77.59796  55.      ]
#  [ 1.        0.        0.       ... 12.274538 76.644605 51.      ]
#  [ 0.        0.        0.       ... 12.778033 77.632191 43.      ]
#  ...
#  [ 1.        0.        0.       ... 26.928785 75.828002 27.1     ]
#  [ 0.        0.        0.       ... 12.90015  80.22791  67.      ]
#  [ 1.        0.        1.       ... 26.832353 75.841749 27.8     ]]
# <class 'numpy.ndarray'>
# (29451, 11)

# In[12]:


ratio = 0.8
offset = int(data_train.shape[0] * ratio)
train_data = data_train[:offset]
test_data = data_train[offset:]
train_data.shape


# In[13]:


def read_data(data_set):
    """
    一个reader
    Args：
        data_set -- 要获取的数据集
    Return：
        reader -- 用于获取训练集及其标签的生成器generator
    """
    def reader():
        """
        一个reader
        Args：
        Return：
            data[:-1],data[-1:] --使用yield返回生成器
                data[:-1]表示前n-1个元素，也就是训练数据，
                data[-1:]表示最后一个元素，也就是对应的标签
        """
        for data in data_set:
            yield data[:-1],data[-1:]
    return reader
 
#测试reader
 
test_array = train_data
print("test_array for read_data:")
for value in read_data(test_array)():
    print(value)
 


# In[14]:


BATCH_SIZE = 8
 
# 设置训练reader
train_reader = paddle.batch(
    paddle.reader.shuffle(
        read_data(train_data), 
        buf_size=500),
    batch_size=BATCH_SIZE)
 
#设置测试 reader
test_reader = paddle.batch(
    paddle.reader.shuffle(
        read_data(test_data), 
        buf_size=500),
    batch_size=BATCH_SIZE)


# 

# In[15]:


feature_num = 11
training_data = data_train
# test_data = data_test
# 计算train 数据集的最大值，最小值, 平均值
maximums, minimums, avgs = data_train.max(axis=0),                data_train.min(axis=0),                data_train.sum(axis=0) / data_train.shape[0]

# 对数据进行归一化处理(包含了训练数据和测试数据)
for i in range(feature_num):
    training_data[:, i] = (data_train[:, i] - avgs[i]) / (maximums[i] - minimums[i])
print(training_data)
print(type(training_data))
print(np.shape(training_data))
# 划分训练集和测试集，使用80%的数据进行训练，20%数据进行测试，训练和测试数据无交集
ratio = 0.8
offset = int(training_data.shape[0] * ratio)
train_data = training_data[:offset]
test_data = training_data[offset:]
#打印训练集
print(np.shape(train_data))


# In[16]:


#对数据进行特征与标签的划分
x = train_data[:, :-1]
y = train_data[:, -1:]
print(x[0], y[0])


# In[17]:


#使用CPU或者GPU训练
use_cuda = False
place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace() 


# In[18]:


paddle.enable_static()


# 

# In[19]:


# 输入层，fluid.layers.data表示数据层,name=’x’：名称为x,输出类型为tensor
# shape=[1]:数据为1维向量
# dtype='float32'：数据类型为float32
 
x = fluid.layers.data(name='x', shape=[10], dtype='float32')
 
 
# 标签数据，fluid.layers.data表示数据层,name=’y’：名称为y,输出类型为tensor
# shape=[1]:数据为1维向量
y = fluid.layers.data(name='y', shape=[1], dtype = 'float32')
 
# 输出层，fluid.layers.fc表示全连接层，input=x: 该层输入数据为x
# size=1：神经元个数，act=None：激活函数为线性函数
y_predict = fluid.layers.fc(input=x, size=1, act=None)


# In[20]:


# 定义损失函数为均方差损失函数,并且求平均损失，返回值名称为avg_loss
avg_loss = fluid.layers.square_error_cost(input = y_predict, label = y)
avg_loss = fluid.layers.mean(avg_loss)
 


# In[21]:


exe = fluid.Executor(place)


# In[22]:


main_program = fluid.default_main_program() # 获取默认/全局主函数
startup_program = fluid.default_startup_program() # 获取默认/全局启动程序
 
#克隆main_program得到test_program
#有些operator在训练和测试之间的操作是不同的，例如batch_norm，使用参数for_test来区分该程序是用来训练还是用来测试
#该api不会删除任何操作符,请在backward和optimization之前使用
test_program = main_program.clone(for_test=True)


# In[23]:


# 创建optimizer，更多优化算子可以参考 fluid.optimizer()
learning_rate = 0.0005
sgd_optimizer = fluid.optimizer.SGD(learning_rate)
sgd_optimizer.minimize(avg_loss)
print("optimizer is ready")


# In[24]:


# For training test cost
def train_test(executor, program, reader, feeder, fetch_list):
    accumulated = 1 * [0]
    count = 0
    for test_data in reader():
        outs = executor.run(
            program=program, feed=feeder.feed(test_data), fetch_list=fetch_list)
        accumulated = [x_c[0] + x_c[1][0] for x_c in zip(accumulated, outs)]  # 累加测试过程中的损失值
        count += 1 # 累加测试集中的样本数量
    return [x_d / count for x_d in accumulated] # 计算平均损失
 
#定义模型保存路径：
#params_dirname用于定义模型保存路径。
params_dirname = "easy_fit_a_line.inference.model"


# 

# In[25]:


#定义张量变量x，表示13维的特征值
x = fluid.layers.data(name='x', shape=[10], dtype='float32')
#定义张量y,表示目标值
y = fluid.layers.data(name='y', shape=[1], dtype='float32')
#定义一个简单的线性网络,连接输入和输出的全连接层
#input:输入tensor;
#size:该层输出单元的数目
#act:激活函数
y_predict=fluid.layers.fc(input=x,size=1,act=None)


# In[26]:


cost = fluid.layers.square_error_cost(input=y_predict, label=y) #求一个batch的损失值
avg_cost = fluid.layers.mean(cost)  


# In[27]:


optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001)
opts = optimizer.minimize(avg_cost)


# In[ ]:


# 数据集介绍可参考该内容（数据来源、归类领域、数据类型、保存格式、样本数量、类别等具体信息）：
# https://aistudio.baidu.com/aistudio/competition/detail/63/0/task-definition


# In[28]:


test_program = fluid.default_main_program().clone(for_test=True)


# 

# In[29]:



use_cuda = False                         #use_cuda为False,表示运算场所为CPU;use_cuda为True,表示运算场所为GPU           
place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
exe = fluid.Executor(place)              #创建一个Executor实例exe
exe.run(fluid.default_startup_program()) #Executor的run()方法执行startup_program(),进行参数初始化


# In[30]:


print(x.shape)


# In[31]:


# 定义输入数据维度
feeder = fluid.DataFeeder(place=place, feed_list=[x, y])#feed_list:向模型输入的变量表或变量表名


# In[32]:


iter=0;
iters=[]
train_costs=[]

def draw_train_process(iters,train_costs):
    title="training cost"
    plt.title(title, fontsize=24)
    plt.xlabel("iter", fontsize=14)
    plt.ylabel("cost", fontsize=14)
    plt.plot(iters, train_costs,color='red',label='training cost') 
    plt.grid()
    plt.show()


# In[33]:


EPOCH_NUM=10
model_save_dir = "/home/aistudio/work/fit_a_line.inference.model"

for pass_id in range(EPOCH_NUM):                                  #训练EPOCH_NUM轮
    # 开始训练并输出最后一个batch的损失值
    train_cost = 0
    for batch_id, data in enumerate(train_reader()):              #遍历train_reader迭代器
        train_cost = exe.run(program=fluid.default_main_program(),#运行主程序
                             feed=feeder.feed(data),              #喂入一个batch的训练数据，根据feed_list和data提供的信息，将输入数据转成一种特殊的数据结构
                             fetch_list=[avg_cost])    
        if batch_id % 40 == 0:
            print("Pass:%d, Cost:%0.5f" % (pass_id, train_cost[0][0]))    #打印最后一个batch的损失值
        iter=iter+BATCH_SIZE
        iters.append(iter)
        train_costs.append(train_cost[0][0])
       
   
    # 开始测试并输出最后一个batch的损失值
    test_cost = 0
    for batch_id, data in enumerate(test_reader()):               #遍历test_reader迭代器
        test_cost= exe.run(program=test_program, #运行测试cheng
                            feed=feeder.feed(data),               #喂入一个batch的测试数据
                            fetch_list=[avg_cost])                #fetch均方误差
    print('Test:%d, Cost:%0.5f' % (pass_id, test_cost[0][0]))     #打印最后一个batch的损失值


    #保存模型
    # 如果保存路径不存在就创建
if not os.path.exists(model_save_dir):
    os.makedirs(model_save_dir)
print ('save models to %s' % (model_save_dir))
#保存训练参数到指定路径中，构建一个专门用预测的program
fluid.io.save_inference_model(model_save_dir,   #保存推理model的路径
                                  ['x'],            #推理（inference）需要 feed 的数据
                                  [y_predict],      #保存推理（inference）结果的 Variables
                                  exe)              #exe 保存 inference model
draw_train_process(iters,train_costs)


# In[ ]:





# 

# In[ ]:





# 

# In[ ]:





# In[ ]:





# ---
